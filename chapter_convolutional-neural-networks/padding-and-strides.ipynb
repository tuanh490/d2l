{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ca8e5cd",
      "metadata": {
        "origin_pos": 1,
        "id": "1ca8e5cd"
      },
      "source": [
        "# Padding and Stride\n",
        ":label:`sec_padding`\n",
        "\n",
        "Recall the example of a convolution in :numref:`fig_correlation`.\n",
        "The input had both a height and width of 3\n",
        "and the convolution kernel had both a height and width of 2,\n",
        "yielding an output representation with dimension $2\\times2$.\n",
        "Assuming that the input shape is $n_\\textrm{h}\\times n_\\textrm{w}$\n",
        "and the convolution kernel shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\n",
        "the output shape will be $(n_\\textrm{h}-k_\\textrm{h}+1) \\times (n_\\textrm{w}-k_\\textrm{w}+1)$:\n",
        "we can only shift the convolution kernel so far until it runs out\n",
        "of pixels to apply the convolution to.\n",
        "\n",
        "In the following we will explore a number of techniques,\n",
        "including padding and strided convolutions,\n",
        "that offer more control over the size of the output.\n",
        "As motivation, note that since kernels generally\n",
        "have width and height greater than $1$,\n",
        "after applying many successive convolutions,\n",
        "we tend to wind up with outputs that are\n",
        "considerably smaller than our input.\n",
        "If we start with a $240 \\times 240$ pixel image,\n",
        "ten layers of $5 \\times 5$ convolutions\n",
        "reduce the image to $200 \\times 200$ pixels,\n",
        "slicing off $30 \\%$ of the image and with it\n",
        "obliterating any interesting information\n",
        "on the boundaries of the original image.\n",
        "*Padding* is the most popular tool for handling this issue.\n",
        "In other cases, we may want to reduce the dimensionality drastically,\n",
        "e.g., if we find the original input resolution to be unwieldy.\n",
        "*Strided convolutions* are a popular technique that can help in these instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49dab7e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:14.385177Z",
          "iopub.status.busy": "2023-08-18T19:43:14.384720Z",
          "iopub.status.idle": "2023-08-18T19:43:16.173429Z",
          "shell.execute_reply": "2023-08-18T19:43:16.172321Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "49dab7e8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf1f4a4",
      "metadata": {
        "origin_pos": 6,
        "id": "4bf1f4a4"
      },
      "source": [
        "## Padding\n",
        "\n",
        "As described above, one tricky issue when applying convolutional layers\n",
        "is that we tend to lose pixels on the perimeter of our image. Consider :numref:`img_conv_reuse` that depicts the pixel utilization as a function of the convolution kernel size and the position within the image. The pixels in the corners are hardly used at all.\n",
        "\n",
        "![Pixel utilization for convolutions of size $1 \\times 1$, $2 \\times 2$, and $3 \\times 3$ respectively.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-reuse.svg?raw=1)\n",
        ":label:`img_conv_reuse`\n",
        "\n",
        "Since we typically use small kernels,\n",
        "for any given convolution\n",
        "we might only lose a few pixels\n",
        "but this can add up as we apply\n",
        "many successive convolutional layers.\n",
        "One straightforward solution to this problem\n",
        "is to add extra pixels of filler around the boundary of our input image,\n",
        "thus increasing the effective size of the image.\n",
        "Typically, we set the values of the extra pixels to zero.\n",
        "In :numref:`img_conv_pad`, we pad a $3 \\times 3$ input,\n",
        "increasing its size to $5 \\times 5$.\n",
        "The corresponding output then increases to a $4 \\times 4$ matrix.\n",
        "The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
        "\n",
        "![Two-dimensional cross-correlation with padding.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-pad.svg?raw=1)\n",
        ":label:`img_conv_pad`\n",
        "\n",
        "In general, if we add a total of $p_\\textrm{h}$ rows of padding\n",
        "(roughly half on top and half on bottom)\n",
        "and a total of $p_\\textrm{w}$ columns of padding\n",
        "(roughly half on the left and half on the right),\n",
        "the output shape will be\n",
        "\n",
        "$$(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+1)\\times(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+1).$$\n",
        "\n",
        "This means that the height and width of the output\n",
        "will increase by $p_\\textrm{h}$ and $p_\\textrm{w}$, respectively.\n",
        "\n",
        "In many cases, we will want to set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$\n",
        "to give the input and output the same height and width.\n",
        "This will make it easier to predict the output shape of each layer\n",
        "when constructing the network.\n",
        "Assuming that $k_\\textrm{h}$ is odd here,\n",
        "we will pad $p_\\textrm{h}/2$ rows on both sides of the height.\n",
        "If $k_\\textrm{h}$ is even, one possibility is to\n",
        "pad $\\lceil p_\\textrm{h}/2\\rceil$ rows on the top of the input\n",
        "and $\\lfloor p_\\textrm{h}/2\\rfloor$ rows on the bottom.\n",
        "We will pad both sides of the width in the same way.\n",
        "\n",
        "CNNs commonly use convolution kernels\n",
        "with odd height and width values, such as 1, 3, 5, or 7.\n",
        "Choosing odd kernel sizes has the benefit\n",
        "that we can preserve the dimensionality\n",
        "while padding with the same number of rows on top and bottom,\n",
        "and the same number of columns on left and right.\n",
        "\n",
        "Moreover, this practice of using odd kernels\n",
        "and padding to precisely preserve dimensionality\n",
        "offers a clerical benefit.\n",
        "For any two-dimensional tensor `X`,\n",
        "when the kernel's size is odd\n",
        "and the number of padding rows and columns\n",
        "on all sides are the same,\n",
        "thereby producing an output with the same height and width as the input,\n",
        "we know that the output `Y[i, j]` is calculated\n",
        "by cross-correlation of the input and convolution kernel\n",
        "with the window centered on `X[i, j]`.\n",
        "\n",
        "In the following example, we create a two-dimensional convolutional layer\n",
        "with a height and width of 3\n",
        "and (**apply 1 pixel of padding on all sides.**)\n",
        "Given an input with a height and width of 8,\n",
        "we find that the height and width of the output is also 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e8917d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:16.177580Z",
          "iopub.status.busy": "2023-08-18T19:43:16.176879Z",
          "iopub.status.idle": "2023-08-18T19:43:16.211812Z",
          "shell.execute_reply": "2023-08-18T19:43:16.210995Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "b5e8917d",
        "outputId": "f90acbbc-aa47-463c-c207-4621ace2edd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We define a helper function to calculate convolutions. It initializes the\n",
        "# convolutional layer weights and performs corresponding dimensionality\n",
        "# elevations and reductions on the input and output\n",
        "def comp_conv2d(conv2d, X):\n",
        "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    # Strip the first two dimensions: examples and channels\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "# 1 row and column is padded on either side, so a total of 2 rows or columns\n",
        "# are added\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbcc821",
      "metadata": {
        "origin_pos": 11,
        "id": "dbbcc821"
      },
      "source": [
        "When the height and width of the convolution kernel are different,\n",
        "we can make the output and input have the same height and width\n",
        "by [**setting different padding numbers for height and width.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa91aee",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:16.215455Z",
          "iopub.status.busy": "2023-08-18T19:43:16.214828Z",
          "iopub.status.idle": "2023-08-18T19:43:16.221907Z",
          "shell.execute_reply": "2023-08-18T19:43:16.221110Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "1aa91aee",
        "outputId": "06d06e7a-c2d4-4dcf-eeaa-1679fd2a9114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 8])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We use a convolution kernel with height 5 and width 3. The padding on either\n",
        "# side of the height and width are 2 and 1, respectively\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cad6e56",
      "metadata": {
        "origin_pos": 16,
        "id": "8cad6e56"
      },
      "source": [
        "## Stride\n",
        "\n",
        "When computing the cross-correlation,\n",
        "we start with the convolution window\n",
        "at the upper-left corner of the input tensor,\n",
        "and then slide it over all locations both down and to the right.\n",
        "In the previous examples, we defaulted to sliding one element at a time.\n",
        "However, sometimes, either for computational efficiency\n",
        "or because we wish to downsample,\n",
        "we move our window more than one element at a time,\n",
        "skipping the intermediate locations. This is particularly useful if the convolution\n",
        "kernel is large since it captures a large area of the underlying image.\n",
        "\n",
        "We refer to the number of rows and columns traversed per slide as *stride*.\n",
        "So far, we have used strides of 1, both for height and width.\n",
        "Sometimes, we may want to use a larger stride.\n",
        ":numref:`img_conv_stride` shows a two-dimensional cross-correlation operation\n",
        "with a stride of 3 vertically and 2 horizontally.\n",
        "The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
        "We can see that when the second element of the first column is generated,\n",
        "the convolution window slides down three rows.\n",
        "The convolution window slides two columns to the right\n",
        "when the second element of the first row is generated.\n",
        "When the convolution window continues to slide two columns to the right on the input,\n",
        "there is no output because the input element cannot fill the window\n",
        "(unless we add another column of padding).\n",
        "\n",
        "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-stride.svg?raw=1)\n",
        ":label:`img_conv_stride`\n",
        "\n",
        "In general, when the stride for the height is $s_\\textrm{h}$\n",
        "and the stride for the width is $s_\\textrm{w}$, the output shape is\n",
        "\n",
        "$$\\lfloor(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+s_\\textrm{h})/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+s_\\textrm{w})/s_\\textrm{w}\\rfloor.$$\n",
        "\n",
        "If we set $p_\\textrm{h}=k_\\textrm{h}-1$ and $p_\\textrm{w}=k_\\textrm{w}-1$,\n",
        "then the output shape can be simplified to\n",
        "$\\lfloor(n_\\textrm{h}+s_\\textrm{h}-1)/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}+s_\\textrm{w}-1)/s_\\textrm{w}\\rfloor$.\n",
        "Going a step further, if the input height and width\n",
        "are divisible by the strides on the height and width,\n",
        "then the output shape will be $(n_\\textrm{h}/s_\\textrm{h}) \\times (n_\\textrm{w}/s_\\textrm{w})$.\n",
        "\n",
        "Below, we [**set the strides on both the height and width to 2**],\n",
        "thus halving the input height and width.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9ed33d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:16.225546Z",
          "iopub.status.busy": "2023-08-18T19:43:16.225010Z",
          "iopub.status.idle": "2023-08-18T19:43:16.232355Z",
          "shell.execute_reply": "2023-08-18T19:43:16.231524Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "cc9ed33d",
        "outputId": "fd1a465a-d1df-4154-e80d-e9b5a02311b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 4])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe567d48",
      "metadata": {
        "origin_pos": 21,
        "id": "fe567d48"
      },
      "source": [
        "Let's look at (**a slightly more complicated example**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530a0750",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:43:16.235915Z",
          "iopub.status.busy": "2023-08-18T19:43:16.235234Z",
          "iopub.status.idle": "2023-08-18T19:43:16.243134Z",
          "shell.execute_reply": "2023-08-18T19:43:16.242281Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "530a0750",
        "outputId": "32e3c0d5-f470-4772-ba59-0ab0e02747f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1bac70f",
      "metadata": {
        "origin_pos": 26,
        "id": "d1bac70f"
      },
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input to avoid undesirable shrinkage of the output. Moreover, it ensures that all pixels are used equally frequently. Typically we pick symmetric padding on both sides of the input height and width. In this case we refer to $(p_\\textrm{h}, p_\\textrm{w})$ padding. Most commonly we set $p_\\textrm{h} = p_\\textrm{w}$, in which case we simply state that we choose padding $p$.\n",
        "\n",
        "A similar convention applies to strides. When horizontal stride $s_\\textrm{h}$ and vertical stride $s_\\textrm{w}$ match, we simply talk about stride $s$. The stride can reduce the resolution of the output, for example reducing the height and width of the output to only $1/n$ of the height and width of the input for $n > 1$. By default, the padding is 0 and the stride is 1.\n",
        "\n",
        "So far all padding that we discussed simply extended images with zeros. This has significant computational benefit since it is trivial to accomplish. Moreover, operators can be engineered to take advantage of this padding implicitly without the need to allocate additional memory. At the same time, it allows CNNs to encode implicit position information within an image, simply by learning where the \"whitespace\" is. There are many alternatives to zero-padding. :citet:`Alsallakh.Kokhlikyan.Miglani.ea.2020` provided an extensive overview of those (albeit without a clear case for when to use nonzero paddings unless artifacts occur).\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Given the final code example in this section with kernel size $(3, 5)$, padding $(0, 1)$, and stride $(3, 4)$,\n",
        "   calculate the output shape to check if it is consistent with the experimental result.\n",
        "1. For audio signals, what does a stride of 2 correspond to?\n",
        "1. Implement mirror padding, i.e., padding where the border values are simply mirrored to extend tensors.\n",
        "1. What are the computational benefits of a stride larger than 1?\n",
        "1. What might be statistical benefits of a stride larger than 1?\n",
        "1. How would you implement a stride of $\\frac{1}{2}$? What does it correspond to? When would this be useful?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49d246dc",
      "metadata": {
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "49d246dc"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/68)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1:\n",
        "\n",
        "We have:\n",
        "\n",
        "$\n",
        "n_h = n_w = 8 \\\\\n",
        "k_h = 3, k_w = 5 \\\\\n",
        "p_h = 0, p_w = 1 \\\\\n",
        "s_h = 3, s_w = 4 \\\\\n",
        "$\n",
        "\n",
        "Hence, the shape is:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "& \\lfloor(n_\\textrm{h}-k_\\textrm{h}+p_\\textrm{h}+s_\\textrm{h})/s_\\textrm{h}\\rfloor \\times \\lfloor(n_\\textrm{w}-k_\\textrm{w}+p_\\textrm{w}+s_\\textrm{w})/s_\\textrm{w}\\rfloor \\\\\n",
        "= & \\lfloor(8 - 3 + 0 + 3)/3 \\rfloor \\times \\lfloor (8 - 5 + 1 + 4)/4 \\rfloor \\\\\n",
        "= & \\ 2 \\times 2\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "which is consistent with the experimental result."
      ],
      "metadata": {
        "id": "IabUIq3uMWoY"
      },
      "id": "IabUIq3uMWoY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2:\n",
        "\n",
        "In audio signal processing, a **stride of 2** typically refers to the step size when applying a sliding window operation, such as in convolution or feature extraction.\n",
        "\n",
        "1. **Downsampling by a Factor of 2**\n",
        "\n",
        "   - If the stride of 2 is applied during feature extraction (e.g., in a convolutional layer), it means that every second sample is skipped, effectively reducing the temporal resolution of the signal.\n",
        "   - This is equivalent to halving the sampling rate, which results in losing high-frequency components if not properly filtered.\n",
        "\n",
        "2. **Frame Shifting in STFT or Feature Extraction**\n",
        "\n",
        "   - In a Short-Time Fourier Transform (STFT) or Mel spectrogram calculation, a stride of 2 (or hop length of 2) means that the analysis window moves forward by 2 samples at each step.\n",
        "   - This would produce a denser representation compared to a larger stride.\n",
        "\n",
        "3. **Stride in a Convolutional Neural Network (CNN) for Audio**\n",
        "\n",
        "   - If a CNN is processing an audio spectrogram with a convolutional kernel and a stride of 2, the feature map resolution is reduced, capturing larger-scale patterns while reducing computational cost."
      ],
      "metadata": {
        "id": "gn2aOs97SCs0"
      },
      "id": "gn2aOs97SCs0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3:"
      ],
      "metadata": {
        "id": "t-GQrFcuTP0y"
      },
      "id": "t-GQrFcuTP0y"
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(1, 65, 1).reshape(8, 8)\n",
        "\n",
        "X_mirror_padded = nn.functional.pad(X.reshape((1,1,8,8)), (1,1,1,1), mode='reflect').reshape(10, 10)\n",
        "X, X_mirror_padded"
      ],
      "metadata": {
        "id": "kYZI-DDbMWPw",
        "outputId": "4d959da6-14c9-4a10-94e0-bff58c1206fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kYZI-DDbMWPw",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
              "         [ 9, 10, 11, 12, 13, 14, 15, 16],\n",
              "         [17, 18, 19, 20, 21, 22, 23, 24],\n",
              "         [25, 26, 27, 28, 29, 30, 31, 32],\n",
              "         [33, 34, 35, 36, 37, 38, 39, 40],\n",
              "         [41, 42, 43, 44, 45, 46, 47, 48],\n",
              "         [49, 50, 51, 52, 53, 54, 55, 56],\n",
              "         [57, 58, 59, 60, 61, 62, 63, 64]]),\n",
              " tensor([[10,  9, 10, 11, 12, 13, 14, 15, 16, 15],\n",
              "         [ 2,  1,  2,  3,  4,  5,  6,  7,  8,  7],\n",
              "         [10,  9, 10, 11, 12, 13, 14, 15, 16, 15],\n",
              "         [18, 17, 18, 19, 20, 21, 22, 23, 24, 23],\n",
              "         [26, 25, 26, 27, 28, 29, 30, 31, 32, 31],\n",
              "         [34, 33, 34, 35, 36, 37, 38, 39, 40, 39],\n",
              "         [42, 41, 42, 43, 44, 45, 46, 47, 48, 47],\n",
              "         [50, 49, 50, 51, 52, 53, 54, 55, 56, 55],\n",
              "         [58, 57, 58, 59, 60, 61, 62, 63, 64, 63],\n",
              "         [50, 49, 50, 51, 52, 53, 54, 55, 56, 55]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4:\n",
        "\n",
        "A stride larger than 1 offers several computational benefits, particularly in audio signal processing and deep learning applications:\n",
        "\n",
        "1. **Reduced Computational Complexity**  \n",
        "   - By skipping samples or feature map positions, fewer operations (e.g., convolutions or multiplications) are needed, leading to faster processing times and lower computational cost.\n",
        "\n",
        "2. **Lower Memory Usage**  \n",
        "   - Since fewer intermediate results are stored, memory requirements decrease, making it feasible to process large audio datasets or deep neural networks with limited resources.\n",
        "\n",
        "3. **Faster Training and Inference in Neural Networks**  \n",
        "   - In convolutional neural networks (CNNs), using a larger stride reduces the size of the feature maps, leading to fewer parameters and faster forward and backward passes.\n",
        "\n",
        "4. **Efficient Downsampling and Feature Extraction**  \n",
        "   - In audio processing, a larger stride can be used to downsample the signal, preserving important features while discarding redundant information, which is useful for speech and music analysis."
      ],
      "metadata": {
        "id": "RcA_xIpRU27x"
      },
      "id": "RcA_xIpRU27x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5:\n",
        "\n",
        "A stride larger than 1 can provide several statistical benefits in audio signal processing and machine learning:\n",
        "\n",
        "1. **Reduction in Redundancy**  \n",
        "   - Many audio signals exhibit strong temporal correlation, meaning adjacent samples contain similar information. A larger stride helps reduce redundancy, leading to more diverse and independent feature representations.\n",
        "\n",
        "2. **Improved Generalization in Machine Learning Models**  \n",
        "   - By reducing overfitting to fine-grained details, a larger stride forces the model to learn more abstract and robust features rather than memorizing noise or unnecessary high-frequency variations.\n",
        "\n",
        "3. **Smoother Feature Representations**  \n",
        "   - In spectrograms or feature extraction methods like Mel-frequency cepstral coefficients (MFCCs), a larger stride results in smoother temporal or spectral representations, which can be beneficial for downstream tasks like speech recognition.\n",
        "\n",
        "4. **Better Noise Robustness**  \n",
        "   - Since a stride larger than 1 effectively averages out fine-grained variations, it can help mitigate the impact of noise, particularly in environments where background interference is present."
      ],
      "metadata": {
        "id": "x0P5nJ6YVu_8"
      },
      "id": "x0P5nJ6YVu_8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6:\n",
        "\n",
        "### **What is a Stride of 1/2?**  \n",
        "A **stride of 1/2** means moving the window by half a step, effectively overlapping adjacent windows. Since standard convolution or pooling operations in frameworks like PyTorch do not support fractional strides directly, achieving a stride of 1/2 requires **interpolation (upsampling)** before applying the convolution or using **dilated convolutions**.\n",
        "\n",
        "### **What Does It Correspond To?**  \n",
        "A stride of 1/2 corresponds to **upsampling** the input before applying a stride-1 operation. In the time domain, this is equivalent to increasing the sample rate (e.g., from 16 kHz to 32 kHz) and then applying a filter. In feature extraction, it means **denser feature maps** with more temporal resolution.\n",
        "\n",
        "### **When is a Stride of 1/2 Useful?**  \n",
        "1. **Super-resolution of Audio Signals** – Helps reconstruct missing information when increasing sampling rates.  \n",
        "2. **Denoising and Smoothing** – Aids in creating finer-grained representations.  \n",
        "3. **Improving Model Performance in CNNs** – Provides finer spatial resolution, especially for tasks like speech recognition.  \n",
        "4. **Overlapping Frame Analysis** – Used in STFT or Mel spectrograms for smoother feature transitions.  \n"
      ],
      "metadata": {
        "id": "YBNCnjOFWAMt"
      },
      "id": "YBNCnjOFWAMt"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "class HalfStrideConvTranspose(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.conv_transpose = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.conv_transpose(X)\n",
        "\n",
        "model = HalfStrideConvTranspose(1, 1, 3)\n",
        "X = torch.randn(1, 1, 10)\n",
        "output = model(X)\n",
        "X, X.shape, output, output.shape"
      ],
      "metadata": {
        "id": "ANGU4yOjT1XY",
        "outputId": "44e9a7bf-1f03-43de-b293-a9e1a4ff3955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ANGU4yOjT1XY",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380,  0.4617,\n",
              "            0.2674,  0.5349,  0.8094]]]),\n",
              " torch.Size([1, 1, 10]),\n",
              " tensor([[[ 0.6427,  0.6003,  0.6407,  0.0036, -0.0077,  0.6000,  0.4411,\n",
              "            1.5303,  1.5885, -0.0499,  0.2246,  0.8204,  0.7516,  0.5859,\n",
              "            0.6585,  0.7303,  0.7867,  0.8153,  0.9182,  0.4209]]],\n",
              "        grad_fn=<ConvolutionBackward0>),\n",
              " torch.Size([1, 1, 20]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l2enkKJkYAEB"
      },
      "id": "l2enkKJkYAEB",
      "execution_count": 19,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}