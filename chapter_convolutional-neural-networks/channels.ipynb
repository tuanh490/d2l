{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3034489e",
      "metadata": {
        "id": "3034489e"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2395e55a",
      "metadata": {
        "id": "2395e55a",
        "outputId": "e2dd18ef-2ecb-49c6-fe0c-ea0776a13c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l==1.0.3 in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (from d2l==1.0.3) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l==1.0.3) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l==1.0.3) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline==0.1.6->d2l==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l==1.0.3) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l==1.0.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l==1.0.3) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l==1.0.3) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l==1.0.3) (6.4.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l==1.0.3) (3.0.13)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l==1.0.3) (2.18.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l==1.0.3) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (23.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l==1.0.3) (1.2.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from qtconsole->jupyter==1.0.0->d2l==1.0.3) (2.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l==1.0.3) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.23.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l==1.0.3) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l==1.0.3) (4.12.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l==1.0.3) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l==1.0.3) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l==1.0.3) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9eca9b3",
      "metadata": {
        "origin_pos": 1,
        "id": "d9eca9b3"
      },
      "source": [
        "# Multiple Input and Multiple Output Channels\n",
        ":label:`sec_channels`\n",
        "\n",
        "While we described the multiple channels\n",
        "that comprise each image (e.g., color images have the standard RGB channels\n",
        "to indicate the amount of red, green and blue) and convolutional layers for multiple channels in :numref:`subsec_why-conv-channels`,\n",
        "until now, we simplified all of our numerical examples\n",
        "by working with just a single input and a single output channel.\n",
        "This allowed us to think of our inputs, convolution kernels,\n",
        "and outputs each as two-dimensional tensors.\n",
        "\n",
        "When we add channels into the mix,\n",
        "our inputs and hidden representations\n",
        "both become three-dimensional tensors.\n",
        "For example, each RGB input image has shape $3\\times h\\times w$.\n",
        "We refer to this axis, with a size of 3, as the *channel* dimension. The notion of\n",
        "channels is as old as CNNs themselves: for instance LeNet-5 :cite:`LeCun.Jackel.Bottou.ea.1995` uses them.\n",
        "In this section, we will take a deeper look\n",
        "at convolution kernels with multiple input and multiple output channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1b38cfcf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:55.203128Z",
          "iopub.status.busy": "2023-08-18T19:36:55.202806Z",
          "iopub.status.idle": "2023-08-18T19:36:58.529112Z",
          "shell.execute_reply": "2023-08-18T19:36:58.525486Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "1b38cfcf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6974b13b",
      "metadata": {
        "origin_pos": 6,
        "id": "6974b13b"
      },
      "source": [
        "## Multiple Input Channels\n",
        "\n",
        "When the input data contains multiple channels,\n",
        "we need to construct a convolution kernel\n",
        "with the same number of input channels as the input data,\n",
        "so that it can perform cross-correlation with the input data.\n",
        "Assuming that the number of channels for the input data is $c_\\textrm{i}$,\n",
        "the number of input channels of the convolution kernel also needs to be $c_\\textrm{i}$. If our convolution kernel's window shape is $k_\\textrm{h}\\times k_\\textrm{w}$,\n",
        "then, when $c_\\textrm{i}=1$, we can think of our convolution kernel\n",
        "as just a two-dimensional tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$.\n",
        "\n",
        "However, when $c_\\textrm{i}>1$, we need a kernel\n",
        "that contains a tensor of shape $k_\\textrm{h}\\times k_\\textrm{w}$ for *every* input channel. Concatenating these $c_\\textrm{i}$ tensors together\n",
        "yields a convolution kernel of shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$.\n",
        "Since the input and convolution kernel each have $c_\\textrm{i}$ channels,\n",
        "we can perform a cross-correlation operation\n",
        "on the two-dimensional tensor of the input\n",
        "and the two-dimensional tensor of the convolution kernel\n",
        "for each channel, adding the $c_\\textrm{i}$ results together\n",
        "(summing over the channels)\n",
        "to yield a two-dimensional tensor.\n",
        "This is the result of a two-dimensional cross-correlation\n",
        "between a multi-channel input and\n",
        "a multi-input-channel convolution kernel.\n",
        "\n",
        ":numref:`fig_conv_multi_in` provides an example\n",
        "of a two-dimensional cross-correlation with two input channels.\n",
        "The shaded portions are the first output element\n",
        "as well as the input and kernel tensor elements used for the output computation:\n",
        "$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$.\n",
        "\n",
        "![Cross-correlation computation with two input channels.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-multi-in.svg?raw=1)\n",
        ":label:`fig_conv_multi_in`\n",
        "\n",
        "\n",
        "To make sure we really understand what is going on here,\n",
        "we can (**implement cross-correlation operations with multiple input channels**) ourselves.\n",
        "Notice that all we are doing is performing a cross-correlation operation\n",
        "per channel and then adding up the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c00d6d37",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.533726Z",
          "iopub.status.busy": "2023-08-18T19:36:58.533243Z",
          "iopub.status.idle": "2023-08-18T19:36:58.538930Z",
          "shell.execute_reply": "2023-08-18T19:36:58.537910Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "c00d6d37"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in(X, K):\n",
        "    # Iterate through the 0th dimension (channel) of K first, then add them up\n",
        "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9b6556",
      "metadata": {
        "origin_pos": 9,
        "id": "ff9b6556"
      },
      "source": [
        "We can construct the input tensor `X` and the kernel tensor `K`\n",
        "corresponding to the values in :numref:`fig_conv_multi_in`\n",
        "to (**validate the output**) of the cross-correlation operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4a511891",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.542978Z",
          "iopub.status.busy": "2023-08-18T19:36:58.542044Z",
          "iopub.status.idle": "2023-08-18T19:36:58.558450Z",
          "shell.execute_reply": "2023-08-18T19:36:58.555512Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "4a511891",
        "outputId": "ede2ca9e-fd8a-4a3c-fbac-5c5a115dd9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  72.],\n",
              "        [104., 120.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f543682c",
      "metadata": {
        "origin_pos": 11,
        "id": "f543682c"
      },
      "source": [
        "## Multiple Output Channels\n",
        ":label:`subsec_multi-output-channels`\n",
        "\n",
        "Regardless of the number of input channels,\n",
        "so far we always ended up with one output channel.\n",
        "However, as we discussed in :numref:`subsec_why-conv-channels`,\n",
        "it turns out to be essential to have multiple channels at each layer.\n",
        "In the most popular neural network architectures,\n",
        "we actually increase the channel dimension\n",
        "as we go deeper in the neural network,\n",
        "typically downsampling to trade off spatial resolution\n",
        "for greater *channel depth*.\n",
        "Intuitively, you could think of each channel\n",
        "as responding to a different set of features.\n",
        "The reality is a bit more complicated than this. A naive interpretation would suggest\n",
        "that representations are learned independently per pixel or per channel.\n",
        "Instead, channels are optimized to be jointly useful.\n",
        "This means that rather than mapping a single channel to an edge detector, it may simply mean\n",
        "that some direction in channel space corresponds to detecting edges.\n",
        "\n",
        "Denote by $c_\\textrm{i}$ and $c_\\textrm{o}$ the number\n",
        "of input and output channels, respectively,\n",
        "and by $k_\\textrm{h}$ and $k_\\textrm{w}$ the height and width of the kernel.\n",
        "To get an output with multiple channels,\n",
        "we can create a kernel tensor\n",
        "of shape $c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$\n",
        "for *every* output channel.\n",
        "We concatenate them on the output channel dimension,\n",
        "so that the shape of the convolution kernel\n",
        "is $c_\\textrm{o}\\times c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$.\n",
        "In cross-correlation operations,\n",
        "the result on each output channel is calculated\n",
        "from the convolution kernel corresponding to that output channel\n",
        "and takes input from all channels in the input tensor.\n",
        "\n",
        "We implement a cross-correlation function\n",
        "to [**calculate the output of multiple channels**] as shown below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9f9f6128",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.562547Z",
          "iopub.status.busy": "2023-08-18T19:36:58.561741Z",
          "iopub.status.idle": "2023-08-18T19:36:58.567371Z",
          "shell.execute_reply": "2023-08-18T19:36:58.566249Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "9f9f6128"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out(X, K):\n",
        "    # Iterate through the 0th dimension of K, and each time, perform\n",
        "    # cross-correlation operations with input X. All of the results are\n",
        "    # stacked together\n",
        "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8dceb2a",
      "metadata": {
        "origin_pos": 13,
        "id": "d8dceb2a"
      },
      "source": [
        "We construct a trivial convolution kernel with three output channels\n",
        "by concatenating the kernel tensor for `K` with `K+1` and `K+2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "27621226",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.571512Z",
          "iopub.status.busy": "2023-08-18T19:36:58.570515Z",
          "iopub.status.idle": "2023-08-18T19:36:58.579033Z",
          "shell.execute_reply": "2023-08-18T19:36:58.578147Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "27621226",
        "outputId": "b90ceff3-7d2c-46d5-e40e-b434a0cfa990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "K = torch.stack((K, K + 1, K + 2), 0)\n",
        "K.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae4d263",
      "metadata": {
        "origin_pos": 15,
        "id": "eae4d263"
      },
      "source": [
        "Below, we perform cross-correlation operations\n",
        "on the input tensor `X` with the kernel tensor `K`.\n",
        "Now the output contains three channels.\n",
        "The result of the first channel is consistent\n",
        "with the result of the previous input tensor `X`\n",
        "and the multi-input channel,\n",
        "single-output channel kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4d36175c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.583043Z",
          "iopub.status.busy": "2023-08-18T19:36:58.582466Z",
          "iopub.status.idle": "2023-08-18T19:36:58.596203Z",
          "shell.execute_reply": "2023-08-18T19:36:58.593357Z"
        },
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "4d36175c",
        "outputId": "33f85f26-1d8e-41c9-bf3f-9ca37513db9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 56.,  72.],\n",
              "         [104., 120.]],\n",
              "\n",
              "        [[ 76., 100.],\n",
              "         [148., 172.]],\n",
              "\n",
              "        [[ 96., 128.],\n",
              "         [192., 224.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "corr2d_multi_in_out(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cccb95",
      "metadata": {
        "origin_pos": 17,
        "id": "43cccb95"
      },
      "source": [
        "## $1\\times 1$ Convolutional Layer\n",
        ":label:`subsec_1x1`\n",
        "\n",
        "At first, a [**$1 \\times 1$ convolution**], i.e., $k_\\textrm{h} = k_\\textrm{w} = 1$,\n",
        "does not seem to make much sense.\n",
        "After all, a convolution correlates adjacent pixels.\n",
        "A $1 \\times 1$ convolution obviously does not.\n",
        "Nonetheless, they are popular operations that are sometimes included\n",
        "in the designs of complex deep networks :cite:`Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017`.\n",
        "Let's see in some detail what it actually does.\n",
        "\n",
        "Because the minimum window is used,\n",
        "the $1\\times 1$ convolution loses the ability\n",
        "of larger convolutional layers\n",
        "to recognize patterns consisting of interactions\n",
        "among adjacent elements in the height and width dimensions.\n",
        "The only computation of the $1\\times 1$ convolution occurs\n",
        "on the channel dimension.\n",
        "\n",
        ":numref:`fig_conv_1x1` shows the cross-correlation computation\n",
        "using the $1\\times 1$ convolution kernel\n",
        "with 3 input channels and 2 output channels.\n",
        "Note that the inputs and outputs have the same height and width.\n",
        "Each element in the output is derived\n",
        "from a linear combination of elements *at the same position*\n",
        "in the input image.\n",
        "You could think of the $1\\times 1$ convolutional layer\n",
        "as constituting a fully connected layer applied at every single pixel location\n",
        "to transform the $c_\\textrm{i}$ corresponding input values into $c_\\textrm{o}$ output values.\n",
        "Because this is still a convolutional layer,\n",
        "the weights are tied across pixel location.\n",
        "Thus the $1\\times 1$ convolutional layer requires $c_\\textrm{o}\\times c_\\textrm{i}$ weights\n",
        "(plus the bias). Also note that convolutional layers are typically followed\n",
        "by nonlinearities. This ensures that $1 \\times 1$ convolutions cannot simply be\n",
        "folded into other convolutions.\n",
        "\n",
        "![The cross-correlation computation uses the $1\\times 1$ convolution kernel with three input channels and two output channels. The input and output have the same height and width.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/conv-1x1.svg?raw=1)\n",
        ":label:`fig_conv_1x1`\n",
        "\n",
        "Let's check whether this works in practice:\n",
        "we implement a $1 \\times 1$ convolution\n",
        "using a fully connected layer.\n",
        "The only thing is that we need to make some adjustments\n",
        "to the data shape before and after the matrix multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a681897",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.600014Z",
          "iopub.status.busy": "2023-08-18T19:36:58.599477Z",
          "iopub.status.idle": "2023-08-18T19:36:58.605934Z",
          "shell.execute_reply": "2023-08-18T19:36:58.604806Z"
        },
        "origin_pos": 18,
        "tab": [
          "pytorch"
        ],
        "id": "6a681897"
      },
      "outputs": [],
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "    c_i, h, w = X.shape\n",
        "    c_o = K.shape[0]\n",
        "    X = X.reshape((c_i, h * w))\n",
        "    K = K.reshape((c_o, c_i))\n",
        "    # Matrix multiplication in the fully connected layer\n",
        "    Y = torch.matmul(K, X)\n",
        "    return Y.reshape((c_o, h, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81851edf",
      "metadata": {
        "origin_pos": 19,
        "id": "81851edf"
      },
      "source": [
        "When performing $1\\times 1$ convolutions,\n",
        "the above function is equivalent to the previously implemented cross-correlation function `corr2d_multi_in_out`.\n",
        "Let's check this with some sample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e0542628",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:36:58.612710Z",
          "iopub.status.busy": "2023-08-18T19:36:58.610896Z",
          "iopub.status.idle": "2023-08-18T19:36:58.627437Z",
          "shell.execute_reply": "2023-08-18T19:36:58.626346Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "e0542628"
      },
      "outputs": [],
      "source": [
        "X = torch.normal(0, 1, (3, 3, 3))\n",
        "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_in_out(X, K)\n",
        "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ececf943",
      "metadata": {
        "origin_pos": 23,
        "id": "ececf943"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for *localized* analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time. They also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision.\n",
        "\n",
        "Note, though, that this flexibility comes at a price. Given an image of size $(h \\times w)$, the cost for computing a $k \\times k$ convolution is $\\mathcal{O}(h \\cdot w \\cdot k^2)$. For $c_\\textrm{i}$ and $c_\\textrm{o}$ input and output channels respectively this increases to $\\mathcal{O}(h \\cdot w \\cdot k^2 \\cdot c_\\textrm{i} \\cdot c_\\textrm{o})$. For a $256 \\times 256$ pixel image with a $5 \\times 5$ kernel and $128$ input and output channels respectively this amounts to over 53 billion operations (we count multiplications and additions separately). Later on we will encounter effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to be block-diagonal, leading to architectures such as ResNeXt :cite:`Xie.Girshick.Dollar.ea.2017`.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Assume that we have two convolution kernels of size $k_1$ and $k_2$, respectively\n",
        "   (with no nonlinearity in between).\n",
        "    1. Prove that the result of the operation can be expressed by a single convolution.\n",
        "    1. What is the dimensionality of the equivalent single convolution?\n",
        "    1. Is the converse true, i.e., can you always decompose a convolution into two smaller ones?\n",
        "1. Assume an input of shape $c_\\textrm{i}\\times h\\times w$ and a convolution kernel of shape\n",
        "   $c_\\textrm{o}\\times c_\\textrm{i}\\times k_\\textrm{h}\\times k_\\textrm{w}$, padding of $(p_\\textrm{h}, p_\\textrm{w})$, and stride of $(s_\\textrm{h}, s_\\textrm{w})$.\n",
        "    1. What is the computational cost (multiplications and additions) for the forward propagation?\n",
        "    1. What is the memory footprint?\n",
        "    1. What is the memory footprint for the backward computation?\n",
        "    1. What is the computational cost for the backpropagation?\n",
        "1. By what factor does the number of calculations increase if we double both the number of input channels\n",
        "   $c_\\textrm{i}$ and the number of output channels $c_\\textrm{o}$? What happens if we double the padding?\n",
        "1. Are the variables `Y1` and `Y2` in the final example of this section exactly the same? Why?\n",
        "1. Express convolutions as a matrix multiplication, even when the convolution window is not $1 \\times 1$.\n",
        "1. Your task is to implement fast convolutions with a $k \\times k$ kernel. One of the algorithm candidates\n",
        "   is to scan horizontally across the source, reading a $k$-wide strip and computing the $1$-wide output strip\n",
        "   one value at a time. The alternative is to read a $k + \\Delta$ wide strip and compute a $\\Delta$-wide\n",
        "   output strip. Why is the latter preferable? Is there a limit to how large you should choose $\\Delta$?\n",
        "1. Assume that we have a $c \\times c$ matrix.\n",
        "    1. How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into $b$ blocks?\n",
        "    1. What is the downside of having $b$ blocks? How could you fix it, at least partly?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe6166e",
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "3fe6166e"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1:\n",
        "\n",
        "#### 1.1.\n",
        "Given an input tensor $ x $ and a kernel $ k_1 $, the **cross-correlation** is defined as:\n",
        "\n",
        "$$\n",
        "(y_1)_t = (x \\star k_1)_t = \\sum_{s \\in S} x_{t+s} k_{1s}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ x_t $ is the input signal (or tensor).\n",
        "- $ k_{1s} $ is the first kernel.\n",
        "- The sum runs over all valid indices $ s $ where the kernel is defined.\n",
        "\n",
        "Now, applying a second kernel $ k_2 $ to $ y_1 $:\n",
        "\n",
        "$$\n",
        "(y_2)_u = (y_1 \\star k_2)_u = \\sum_{s \\in S} y_{1,u+s} k_{2s}\n",
        "$$\n",
        "\n",
        "Substituting $ y_{1,u+s} $ from the first step:\n",
        "\n",
        "$$\n",
        "(y_2)_u = \\sum_{s \\in S} \\left( \\sum_{t \\in S} x_{u+s+t} k_{1t} \\right) k_{2s}\n",
        "$$\n",
        "\n",
        "Rearranging the sums:\n",
        "\n",
        "$$\n",
        "(y_2)_u = \\sum_{t \\in S} \\sum_{s \\in S} x_{u+s+t} k_{1t} k_{2s}\n",
        "$$\n",
        "\n",
        "Recognizing the form of cross-correlation, we define an **equivalent kernel**:\n",
        "\n",
        "$4\n",
        "w_{\\text{eq}, v} = \\sum_{s \\in S} k_{1,v-s} k_{2s}\n",
        "$$\n",
        "\n",
        "so that:\n",
        "\n",
        "$$\n",
        "(y_2)_u = \\sum_{v \\in S} x_{u+v} w_{\\text{eq}, v}\n",
        "$$\n",
        "\n",
        "which is precisely a **single cross-correlation** operation:\n",
        "\n",
        "$$\n",
        "y_2 = x \\star w_{\\text{eq}}\n",
        "$$\n",
        "\n",
        "The equivalent kernel $ w_{\\text{eq}} $ is the **cross-correlation** of $ k_1 $ and $ k_2 $:\n",
        "\n",
        "$$\n",
        "w_{\\text{eq}} = k_1 \\star k_2\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "Zk5GUwgfmlTM"
      },
      "id": "Zk5GUwgfmlTM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.\n",
        "The dimensionality of the resulting single convolution is $k_1 + k_2 - 1$"
      ],
      "metadata": {
        "id": "JbIViWxhtE3N"
      },
      "id": "JbIViWxhtE3N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.\n",
        "\n",
        "The converse would state that any convolution with kernel size $k_{eq}$ can always be decomposed into two smaller convolutions of sizes $k_1$ and $k_2$.\n",
        "\n",
        "In general, this is not always possible. The decomposition depends on the structure of the kernel:\n",
        "\n",
        "- If the kernel is separable, meaning it can be factored into two smaller kernels, then decomposition is possible.\n",
        "- However, many kernels are non-separable and do not allow exact decomposition into smaller convolutions."
      ],
      "metadata": {
        "id": "6M5diwVOtiep"
      },
      "id": "6M5diwVOtiep"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2:"
      ],
      "metadata": {
        "id": "c8nMOGx8uUPt"
      },
      "id": "c8nMOGx8uUPt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.\n",
        "\n",
        "$$c_o \\times c_i \\times k_h \\times k_w \\times [(h+p_h-k_h+s_h)//s_h] \\times [(w+p_w-k_w+s_w)//s_w]$$"
      ],
      "metadata": {
        "id": "XIfxMXzMvC7I"
      },
      "id": "XIfxMXzMvC7I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2:\n",
        "\n",
        "We assume an input of shape $ c_i \\times h \\times w $ and a convolution kernel of shape $ c_o \\times c_i \\times k_h \\times k_w $, with padding $ (p_h, p_w) $ and stride $ (s_h, s_w) $.\n",
        "\n",
        "#### **1.1 Forward Propagation Computational Cost**\n",
        "The **output size** is determined by:\n",
        "\n",
        "$$\n",
        "h' = \\frac{h + 2p_h - k_h}{s_h} + 1, \\quad w' = \\frac{w + 2p_w - k_w}{s_w} + 1\n",
        "$$\n",
        "\n",
        "Each output value requires a **dot product** between the kernel and the corresponding input patch. Each dot product involves:\n",
        "\n",
        "$$\n",
        "c_i \\times k_h \\times k_w\n",
        "$$\n",
        "\n",
        "multiplications and an equal number of additions.\n",
        "\n",
        "Since we compute $ c_o $ output channels, and there are $ h' \\times w' $ spatial positions, the total number of multiplications and additions is:\n",
        "\n",
        "$$\n",
        "\\text{Multiplications} = h' w' c_o \\cdot (c_i k_h k_w)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Additions} = h' w' c_o \\cdot (c_i k_h k_w - 1)\n",
        "$$\n",
        "\n",
        "#### **1.2 Forward Memory Footprint**\n",
        "Memory is needed for:\n",
        "- Input: $ c_i \\times h \\times w $\n",
        "- Kernel: $ c_o \\times c_i \\times k_h \\times k_w $\n",
        "- Output: $ c_o \\times h' \\times w' $\n",
        "\n",
        "Thus, the total memory footprint is:\n",
        "\n",
        "$$\n",
        "c_i h w + c_o c_i k_h k_w + c_o h' w'\n",
        "$$\n",
        "\n",
        "#### **1.3 Backward Memory Footprint**\n",
        "For backpropagation, we need to store:\n",
        "- Input $ x $\n",
        "- Gradients $ \\frac{\\partial L}{\\partial x} $ (same size as input)\n",
        "- Gradients $ \\frac{\\partial L}{\\partial w} $ (same size as kernel)\n",
        "- Gradients $ \\frac{\\partial L}{\\partial y} $ (same size as output)\n",
        "\n",
        "Thus, the **backward memory footprint** is:\n",
        "\n",
        "$$\n",
        "2(c_i h w) + (c_o h' w') + (c_o c_i k_h k_w)\n",
        "$$\n",
        "\n",
        "#### **1.4 Computational Cost for Backpropagation**\n",
        "Backpropagation consists of:\n",
        "1. **Gradient w.r.t. input**: This requires convolving $ \\frac{\\partial L}{\\partial y} $ with a **flipped kernel**, leading to:\n",
        "\n",
        "   $$\n",
        "   h w c_i \\cdot (c_o k_h k_w)\n",
        "   $$\n",
        "\n",
        "   multiplications.\n",
        "\n",
        "2. **Gradient w.r.t. kernel**: This involves **convolving** $ x $ with $ \\frac{\\partial L}{\\partial y} $, leading to:\n",
        "\n",
        "   $$\n",
        "   c_o c_i k_h k_w \\cdot (h' w')\n",
        "   $$\n",
        "\n",
        "   multiplications.\n",
        "\n",
        "Thus, the total cost is:\n",
        "\n",
        "$$\n",
        "\\text{Multiplications} = h w c_i (c_o k_h k_w) + c_o c_i k_h k_w (h' w')\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Effect of Doubling $ c_i $, $ c_o $, and Padding**\n",
        "#### **Doubling $ c_i $ and $ c_o $**\n",
        "If both $ c_i $ and $ c_o $ are doubled:\n",
        "- Each kernel grows by $ 2 \\times 2 = 4 $.\n",
        "- The output still requires processing $ h' \\times w' $ patches.\n",
        "\n",
        "The computational cost increases by a factor of **4**:\n",
        "\n",
        "$$\n",
        "\\text{New Cost} = 4 \\times \\text{Original Cost}\n",
        "$$\n",
        "\n",
        "#### **Doubling Padding**\n",
        "Padding affects output size:\n",
        "\n",
        "$$\n",
        "h' = \\frac{h + 4p_h - k_h}{s_h} + 1, \\quad w' = \\frac{w + 4p_w - k_w}{s_w} + 1\n",
        "$$\n",
        "\n",
        "If padding increases but kernel and stride remain the same, the **output size grows**, increasing computations.\n",
        "\n"
      ],
      "metadata": {
        "id": "pdKbd6C3vpwk"
      },
      "id": "pdKbd6C3vpwk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Are $ Y_1 $ and $ Y_2 $ the Same?**\n",
        "Not exactly the same, because the theoretical equivalence holds when the operations are performed with ideal precision and mathematical properties. In practice, especially with finite precision computations, slight differences may arise due to numerical limitations and implementation details.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ii_I24YRyQH1"
      },
      "id": "Ii_I24YRyQH1"
    },
    {
      "cell_type": "code",
      "source": [
        "Y1, Y2"
      ],
      "metadata": {
        "id": "_tCntDcKtC2q",
        "outputId": "5a1c36a9-213c-42d8-e494-f60ab2824b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_tCntDcKtC2q",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.1545e+00,  2.7878e+00,  1.2037e+00],\n",
              "          [-1.0400e+00,  2.0920e+00,  6.2781e-01],\n",
              "          [-6.1205e+00,  4.2483e-03, -4.1907e+00]],\n",
              " \n",
              "         [[ 2.4859e+00,  4.0840e-01, -2.6984e+00],\n",
              "          [ 1.7975e+00, -3.2594e+00,  5.3007e-01],\n",
              "          [ 1.8862e+00,  5.1405e-02,  3.4953e+00]]]),\n",
              " tensor([[[ 1.1545e+00,  2.7878e+00,  1.2037e+00],\n",
              "          [-1.0400e+00,  2.0920e+00,  6.2781e-01],\n",
              "          [-6.1205e+00,  4.2483e-03, -4.1907e+00]],\n",
              " \n",
              "         [[ 2.4859e+00,  4.0840e-01, -2.6984e+00],\n",
              "          [ 1.7975e+00, -3.2594e+00,  5.3007e-01],\n",
              "          [ 1.8862e+00,  5.1405e-02,  3.4953e+00]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **5. Expressing Convolution as Matrix Multiplication**\n",
        "A convolution can be rewritten as a matrix multiplication using the **im2col** trick:\n",
        "1. Unroll input patches into a matrix $ X' $ of shape $ (h' w', c_i k_h k_w) $.\n",
        "2. Reshape kernels into a matrix $ W' $ of shape $ (c_o, c_i k_h k_w) $.\n",
        "3. Perform matrix multiplication:\n",
        "\n",
        "   $$\n",
        "   Y' = W' X'\n",
        "   $$\n",
        "\n",
        "where $ Y' $ is reshaped back into $ (c_o, h', w') $.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Fast Convolutions with a $ k \\times k $ Kernel**\n",
        "Instead of scanning a **1-wide strip**, we can process a **$ k + \\Delta $-wide strip**:\n",
        "\n",
        "- **Advantage**: This reduces redundant memory access and increases cache efficiency.\n",
        "- **Limit**: Large $ \\Delta $ may increase memory overhead and require additional buffering.\n",
        "\n",
        "A good choice of $ \\Delta $ is system-dependent, often optimizing for **cache utilization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Block-Diagonal Matrices for Fast Multiplication**\n",
        "#### **6.1 Speedup with $ b $ Blocks**\n",
        "If a matrix is split into $ b $ **block-diagonal** parts, the speedup is:\n",
        "\n",
        "$$\n",
        "\\frac{c^3}{b (c/b)^3} = b^2\n",
        "$$\n",
        "\n",
        "Multiplication is reduced from $ O(c^3) $ to $ O(b (c/b)^3) = O(c^3/b^2) $.\n",
        "\n",
        "#### **6.2 Downside of Many Blocks**\n",
        "- Increasing $ b $ reduces computational cost **but** increases **communication overhead** between blocks.\n",
        "- Solution: Use **overlapping blocks** or **hierarchical block matrices**."
      ],
      "metadata": {
        "id": "aXMspEMmyS4G"
      },
      "id": "aXMspEMmyS4G"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzPKyQfhx-rS"
      },
      "id": "kzPKyQfhx-rS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}